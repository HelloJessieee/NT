import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class PaperAlignedRiskModel:
    """
    ä¸è®ºæ–‡å®Œå…¨ä¸€è‡´çš„é£é™©é¢„æµ‹æ¨¡å‹
    åŸºäºæ¢¯åº¦æå‡æ¡†æ¶çš„XGBoostæ¨¡å‹
    """
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_importance = None
        self.best_params = None
        self.risk_scores = None
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        try:
            # åŸºç¡€æ•°æ®
            self.pop_df = pd.read_csv("sg_subzone_all_features.csv")
            print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"   åˆ†åŒºæ•°é‡: {len(self.pop_df)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def create_mobility_features(self):
        """åˆ›å»ºäººå£æµåŠ¨ç‰¹å¾ (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸ”„ åˆ›å»ºäººå£æµåŠ¨ç‰¹å¾...")
        
        # è®¡ç®—æ€»æµåŠ¨ (Total Mobility)
        # TM_i = âˆ‘_j OD_ij + âˆ‘_j OD_ji (æµå‡º + æµå…¥)
        df = self.pop_df.copy()
        
        # å‡è®¾æˆ‘ä»¬æœ‰ODçŸ©é˜µæ•°æ®ï¼Œè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä»OD_commuting_matrix.csvåŠ è½½
        np.random.seed(42)
        n_subzones = len(df)
        
        # æ¨¡æ‹ŸODçŸ©é˜µ
        od_matrix = np.random.poisson(100, (n_subzones, n_subzones))
        np.fill_diagonal(od_matrix, 0)  # å¯¹è§’çº¿è®¾ä¸º0
        
        # è®¡ç®—æ€»æµåŠ¨
        total_outflow = od_matrix.sum(axis=1)  # æµå‡º
        total_inflow = od_matrix.sum(axis=0)   # æµå…¥
        total_mobility = total_outflow + total_inflow
        
        # è®¡ç®—æµåŠ¨å¼ºåº¦ (Mobility Intensity)
        # MI_i = TM_i / max(TM)
        mobility_intensity = total_mobility / total_mobility.max()
        
        # è®¡ç®—é£é™©æš´éœ² (Risk Exposure)
        # RE_i = åŸºäºäººå£æµåŠ¨çš„é£é™©æš´éœ²
        risk_exposure = total_mobility * df['Total_Total'] / 1000
        
        df['total_mobility'] = total_mobility
        df['mobility_intensity'] = mobility_intensity
        df['risk_exposure'] = risk_exposure
        
        print(f"   æœ€é«˜æµåŠ¨å¼ºåº¦: {total_mobility.max():.0f}")
        print(f"   å¹³å‡æµåŠ¨å¼ºåº¦: {total_mobility.mean():.0f}")
        
        return df
    
    def create_paper_features(self, df):
        """åˆ›å»ºä¸è®ºæ–‡å®Œå…¨ä¸€è‡´çš„ç‰¹å¾å‘é‡"""
        print("ğŸ”„ åˆ›å»ºè®ºæ–‡ç‰¹å¾å‘é‡...")
        
        # è®ºæ–‡ä¸­çš„ç‰¹å¾å‘é‡: X_i=[P_i, E_i, L_i, H_i, HDB_i, V_i, ED_i, RE_i, TM_i, MI_i]
        
        # P_i: äººå£å¯†åº¦ (Population Density)
        df['P_i'] = df['Total_Total']
        
        # E_i: è€å¹´äººå£æ¯”ä¾‹ (Elderly Share)
        df['E_i'] = df['elderly_ratio']
        
        # L_i: ä½æ”¶å…¥æ¯”ä¾‹ (Low-income Ratio)
        df['L_i'] = df['low_income_ratio']
        
        # H_i: å†å²OHCAå‘ç”Ÿç‡ (Historical OHCA Rate) - ä½¿ç”¨äººå£å¯†åº¦å’Œè€å¹´æ¯”ä¾‹çš„ç»„åˆ
        df['H_i'] = (df['P_i'] * df['E_i'] * 0.001) + np.random.normal(0, 0.01, len(df))
        df['H_i'] = df['H_i'].clip(0, None)  # ç¡®ä¿éè´Ÿ
        
        # HDB_i: ç»„å±‹æ¯”ä¾‹ (HDB Ratio)
        df['HDB_i'] = df['hdb_ratio']
        
        # V_i: ç¤¾ä¼šç»æµè„†å¼±æ€§æŒ‡æ•° (Social Vulnerability Index)
        # V_i = 0.7 * L_i + 0.3 * HDB_i
        df['V_i'] = 0.7 * df['L_i'] + 0.3 * df['HDB_i']
        
        # ED_i: è€å¹´äººå£å¯†åº¦ (Elderly Density)
        # ED_i = E_i * P_i
        df['ED_i'] = df['E_i'] * df['P_i']
        
        # æ·»åŠ éšæœºç”Ÿæˆçš„OHCAæ•°æ®ï¼ˆåŸºäºäººå£å¯†åº¦å’Œè€å¹´æ¯”ä¾‹ï¼Œä½†åŠ å…¥éšæœºæ€§ï¼‰
        np.random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
        base_ohca = df['P_i'] * df['E_i'] * 0.0001  # åŸºäºäººå£å¯†åº¦å’Œè€å¹´æ¯”ä¾‹çš„åŸºç¡€OHCAç‡
        random_factor = np.random.poisson(5, len(df))  # éšæœºå› å­
        df['ohca_count'] = (base_ohca + random_factor).astype(int)
        df['ohca_count'] = df['ohca_count'].clip(0, 1000)  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        df['ohca_rate'] = df['ohca_count'] / df['P_i'].replace(0, 1)  # é¿å…é™¤é›¶
        
        # RE_i: é£é™©æš´éœ² (Risk Exposure) - å·²åœ¨create_mobility_featuresä¸­è®¡ç®—
        # TM_i: æ€»æµåŠ¨ (Total Mobility) - å·²åœ¨create_mobility_featuresä¸­è®¡ç®—
        # MI_i: æµåŠ¨å¼ºåº¦ (Mobility Intensity) - å·²åœ¨create_mobility_featuresä¸­è®¡ç®—
        
        # é‡å‘½åä»¥åŒ¹é…è®ºæ–‡
        df['RE_i'] = df['risk_exposure']
        df['TM_i'] = df['total_mobility']
        df['MI_i'] = df['mobility_intensity']
        
        print("âœ… è®ºæ–‡ç‰¹å¾å‘é‡åˆ›å»ºå®Œæˆ")
        print(f"   ç‰¹å¾: P_i, E_i, L_i, H_i, HDB_i, V_i, ED_i, RE_i, TM_i, MI_i")
        print(f"   OHCAæ•°æ®: éšæœºç”Ÿæˆï¼ŒåŸºäºäººå£å¯†åº¦å’Œè€å¹´æ¯”ä¾‹")
        
        return df
    
    def prepare_features(self):
        """å‡†å¤‡ç‰¹å¾æ•°æ® (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸ”„ å‡†å¤‡ç‰¹å¾æ•°æ®...")
        
        # åˆ›å»ºæµåŠ¨ç‰¹å¾
        df = self.create_mobility_features()
        
        # åˆ›å»ºè®ºæ–‡ç‰¹å¾å‘é‡
        df = self.create_paper_features(df)
        
        # è®ºæ–‡ä¸­çš„ç‰¹å¾åˆ—è¡¨
        feature_columns = ['P_i', 'E_i', 'L_i', 'H_i', 'HDB_i', 'V_i', 'ED_i', 'RE_i', 'TM_i', 'MI_i']
        
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
        available_features = [col for col in feature_columns if col in df.columns]
        missing_features = [col for col in feature_columns if col not in df.columns]
        
        if missing_features:
            print(f"âš ï¸  ç¼ºå¤±ç‰¹å¾: {missing_features}")
            # ç”¨0å¡«å……ç¼ºå¤±ç‰¹å¾
            for feature in missing_features:
                df[feature] = 0
            available_features = feature_columns
        
        self.features = df[available_features]
        self.target = df['ohca_count']
        self.subzone_codes = df['subzone_code']
        
        print(f"âœ… ç‰¹å¾å‡†å¤‡å®Œæˆï¼Œä½¿ç”¨ {len(available_features)} ä¸ªç‰¹å¾")
        print(f"   ç‰¹å¾åˆ—è¡¨: {available_features}")
        
        return df
    
    def optimize_hyperparameters(self):
        """ä¼˜åŒ–è¶…å‚æ•° (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸ”„ ä¼˜åŒ–è¶…å‚æ•°...")
        
        # è®ºæ–‡ä¸­çš„è¶…å‚æ•°è®¾ç½®
        param_grid = {
            'n_estimators': [200],  # è®ºæ–‡ä¸­å›ºå®šä¸º200
            'max_depth': [5],       # è®ºæ–‡ä¸­å›ºå®šä¸º5
            'learning_rate': [0.01, 0.1],
            'subsample': [0.8, 0.9]
        }
        
        # ç½‘æ ¼æœç´¢
        xgb = XGBRegressor(random_state=42, eval_metric='rmse')
        grid_search = GridSearchCV(
            xgb, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1
        )
        
        grid_search.fit(self.features, self.target)
        
        self.best_params = grid_search.best_params_
        print(f"âœ… æœ€ä½³å‚æ•°: {self.best_params}")
        
        return grid_search.best_estimator_
    
    def train_model(self, use_grid_search=True):
        """è®­ç»ƒæ¨¡å‹ (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸ”„ è®­ç»ƒXGBoostæ¨¡å‹...")
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›† (ä¸è®ºæ–‡ä¸€è‡´)
        X_train, X_test, y_train, y_test = train_test_split(
            self.features, self.target, test_size=0.2, random_state=42
        )
        
        if use_grid_search:
            self.model = self.optimize_hyperparameters()
        else:
            # ä½¿ç”¨è®ºæ–‡ä¸­çš„é»˜è®¤å‚æ•°
            self.model = XGBRegressor(
                n_estimators=200,  # è®ºæ–‡ä¸­å›ºå®šä¸º200
                max_depth=5,       # è®ºæ–‡ä¸­å›ºå®šä¸º5
                learning_rate=0.1,
                random_state=42
            )
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = self.model.predict(X_test)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ (ä¸è®ºæ–‡ä¸€è‡´)
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        print(f"   RÂ² Score: {r2:.4f}")
        print(f"   MSE: {mse:.4f}")
        print(f"   MAE: {mae:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§
        self.feature_importance = pd.DataFrame({
            'feature': self.features.columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nğŸ“Š ç‰¹å¾é‡è¦æ€§:")
        for _, row in self.feature_importance.head(5).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")
        
        return r2, mse, mae
    
    def predict_risk_scores(self):
        """é¢„æµ‹é£é™©è¯„åˆ† (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸ”„ é¢„æµ‹é£é™©è¯„åˆ†...")
        
        # å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œé¢„æµ‹
        risk_predictions = self.model.predict(self.features)
        
        # åˆ›å»ºç»“æœDataFrame
        results_df = pd.DataFrame({
            'subzone_code': self.subzone_codes,
            'risk_score': risk_predictions,
            'ohca_count': self.target
        })
        
        # æ ‡å‡†åŒ–é£é™©è¯„åˆ†åˆ°0-1èŒƒå›´
        results_df['risk_score_normalized'] = (
            results_df['risk_score'] - results_df['risk_score'].min()
        ) / (results_df['risk_score'].max() - results_df['risk_score'].min())
        
        self.risk_scores = results_df
        
        print(f"âœ… é£é™©è¯„åˆ†é¢„æµ‹å®Œæˆ")
        print(f"   æœ€é«˜é£é™©åˆ†åŒº: {results_df.loc[results_df['risk_score'].idxmax(), 'subzone_code']}")
        print(f"   å¹³å‡é£é™©è¯„åˆ†: {results_df['risk_score'].mean():.4f}")
        print(f"   é£é™©è¯„åˆ†èŒƒå›´: {results_df['risk_score'].min():.4f} - {results_df['risk_score'].max():.4f}")
        
        return results_df
    
    def save_results(self):
        """ä¿å­˜ç»“æœ (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸ”„ ä¿å­˜ç»“æœ...")
        
        # ä¿å­˜é£é™©è¯„åˆ†
        self.risk_scores.to_csv('outputs/risk_analysis_paper_aligned.csv', index=False)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        self.feature_importance.to_csv('outputs/feature_importance_paper_aligned.csv', index=False)
        
        # ä¿å­˜æ¨¡å‹æ€§èƒ½
        performance_df = pd.DataFrame({
            'metric': ['R2_Score', 'MSE', 'MAE'],
            'value': [self.r2_score, self.mse, self.mae]
        })
        performance_df.to_csv('outputs/model_performance_paper_aligned.csv', index=False)
        
        print("âœ… ç»“æœä¿å­˜å®Œæˆ")
        print("   - outputs/risk_analysis_paper_aligned.csv")
        print("   - outputs/feature_importance_paper_aligned.csv")
        print("   - outputs/model_performance_paper_aligned.csv")
    
    def create_heatmap(self):
        """åˆ›å»ºé£é™©çƒ­åŠ›å›¾ (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸ”„ åˆ›å»ºé£é™©çƒ­åŠ›å›¾...")
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 8))
        
        # ä½¿ç”¨é£é™©è¯„åˆ†åˆ›å»ºé¢œè‰²æ˜ å°„
        risk_values = self.risk_scores['risk_score_normalized'].values
        
        # åˆ›å»ºç½‘æ ¼å¸ƒå±€ (æ¨¡æ‹Ÿåœ°ç†åˆ†å¸ƒ)
        n_subzones = len(risk_values)
        grid_size = int(np.ceil(np.sqrt(n_subzones)))
        
        # åˆ›å»ºç½‘æ ¼çŸ©é˜µ
        grid_matrix = np.zeros((grid_size, grid_size))
        for i in range(n_subzones):
            row = i // grid_size
            col = i % grid_size
            if row < grid_size and col < grid_size:
                grid_matrix[row, col] = risk_values[i]
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(grid_matrix, 
                   cmap='RdYlBu_r', 
                   center=0.5,
                   cbar_kws={'label': 'Normalized Risk Score'},
                   xticklabels=False,
                   yticklabels=False)
        
        plt.title('OHCA Risk Heatmap (Paper-Aligned Model)', fontsize=16)
        plt.xlabel('Geographic Grid (X)', fontsize=12)
        plt.ylabel('Geographic Grid (Y)', fontsize=12)
        
        # ä¿å­˜å›¾ç‰‡
        plt.tight_layout()
        plt.savefig('outputs/risk_heatmap_paper_aligned.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… é£é™©çƒ­åŠ›å›¾åˆ›å»ºå®Œæˆ")
        print("   - outputs/risk_heatmap_paper_aligned.png")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ (ä¸è®ºæ–‡ä¸€è‡´)"""
        print("ğŸš€ å¼€å§‹è¿è¡Œä¸è®ºæ–‡ä¸€è‡´çš„é£é™©æ¨¡å‹åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data():
            return False
        
        # 2. å‡†å¤‡ç‰¹å¾
        self.prepare_features()
        
        # 3. è®­ç»ƒæ¨¡å‹
        self.r2_score, self.mse, self.mae = self.train_model(use_grid_search=False)
        
        # 4. é¢„æµ‹é£é™©è¯„åˆ†
        self.predict_risk_scores()
        
        # 5. åˆ›å»ºçƒ­åŠ›å›¾
        self.create_heatmap()
        
        # 6. ä¿å­˜ç»“æœ
        self.save_results()
        
        print("\nğŸ‰ ä¸è®ºæ–‡ä¸€è‡´çš„é£é™©æ¨¡å‹åˆ†æå®Œæˆï¼")
        print("\nğŸ“Š ä¸»è¦ç»“æœ:")
        print(f"   - æ¨¡å‹æ€§èƒ½: RÂ² = {self.r2_score:.4f}")
        print(f"   - é¢„æµ‹ç²¾åº¦: MSE = {self.mse:.4f}")
        print(f"   - å¹³å‡è¯¯å·®: MAE = {self.mae:.4f}")
        print(f"   - æœ€é«˜é£é™©åˆ†åŒº: {self.risk_scores.loc[self.risk_scores['risk_score'].idxmax(), 'subzone_code']}")
        
        return True

if __name__ == "__main__":
    # è¿è¡Œä¸è®ºæ–‡ä¸€è‡´çš„é£é™©æ¨¡å‹
    model = PaperAlignedRiskModel()
    model.run_complete_analysis() 