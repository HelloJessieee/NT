import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

def load_and_prepare_data():
    """
    åŠ è½½æ•°æ®å¹¶å‡†å¤‡ç‰¹å¾ï¼ŒåŠ å…¥é¢ç§¯æƒé‡
    """
    print("ğŸ”„ åŠ è½½æ•°æ®...")
    
    # è¯»å–æ•°æ®
    data = pd.read_csv("sg_subzone_all_features.csv")
    print(f"âœ… åŠ è½½æ•°æ®: {len(data)} ä¸ªåˆ†åŒº")
    
    # è®¡ç®—é¢ç§¯æƒé‡ï¼ˆä½¿ç”¨äººå£å¯†åº¦ä½œä¸ºä»£ç†ï¼‰
    data['population_density'] = data['Total_Total']
    data['area_weight'] = data['population_density'] / data['population_density'].max()
    
    # å‡†å¤‡ç‰¹å¾
    features = [
        'Total_Total', 'volunteers_count', 'hdb_ratio', 
        'elderly_ratio', 'low_income_ratio', 'AED_count',
        'area_weight'  # æ–°å¢é¢ç§¯æƒé‡ç‰¹å¾
    ]
    
    X = data[features].copy()
    
    # å¤„ç†ç¼ºå¤±å€¼
    X = X.fillna(0)
    
    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæ¨¡æ‹Ÿé£é™©è¯„åˆ†ï¼‰
    # åŸºäºäººå£å¯†åº¦ã€è€å¹´æ¯”ä¾‹ã€ä½æ”¶å…¥æ¯”ä¾‹ç­‰åˆ›å»ºç»¼åˆé£é™©è¯„åˆ†
    risk_factors = (
        X['Total_Total'] * 0.3 +  # äººå£å¯†åº¦æƒé‡
        X['elderly_ratio'] * 10000 * 0.25 +  # è€å¹´æ¯”ä¾‹æƒé‡
        X['low_income_ratio'] * 10000 * 0.25 +  # ä½æ”¶å…¥æ¯”ä¾‹æƒé‡
        (1 - X['hdb_ratio']) * 5000 * 0.1 +  # éHDBæ¯”ä¾‹æƒé‡
        (1 - X['area_weight']) * 3000 * 0.1  # é¢ç§¯æƒé‡ï¼ˆå°é¢ç§¯é«˜é£é™©ï¼‰
    )
    
    # æ·»åŠ éšæœºå™ªå£°ä½¿æ¨¡å‹æ›´çœŸå®
    np.random.seed(42)
    noise = np.random.normal(0, risk_factors.std() * 0.1, len(risk_factors))
    y = risk_factors + noise
    
    # ç¡®ä¿é£é™©è¯„åˆ†ä¸ºæ­£æ•°
    y = np.maximum(y, 0)
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    print(f"   ç‰¹å¾æ•°é‡: {len(features)}")
    print(f"   å¹³å‡é£é™©è¯„åˆ†: {y.mean():.2f}")
    print(f"   é£é™©è¯„åˆ†èŒƒå›´: {y.min():.2f} - {y.max():.2f}")
    
    return X, y, data, features

def train_risk_model(X, y):
    """
    è®­ç»ƒé£é™©é¢„æµ‹æ¨¡å‹
    """
    print("ğŸ”„ è®­ç»ƒé£é™©é¢„æµ‹æ¨¡å‹...")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # åˆ›å»ºXGBoostæ¨¡å‹
    model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        objective='reg:squarederror'
    )
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # è¯„ä¼°æ¨¡å‹
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    
    print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"   è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
    print(f"   æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
    print(f"   è®­ç»ƒé›† RMSE: {train_rmse:.2f}")
    print(f"   æµ‹è¯•é›† RMSE: {test_rmse:.2f}")
    
    return model, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test

def generate_risk_scores(model, X, data):
    """
    ç”Ÿæˆé£é™©è¯„åˆ†
    """
    print("ğŸ”„ ç”Ÿæˆé£é™©è¯„åˆ†...")
    
    # é¢„æµ‹é£é™©è¯„åˆ†
    risk_scores = model.predict(X)
    
    # åˆ›å»ºç»“æœæ•°æ®æ¡†
    result_data = data.copy()
    result_data['risk_score'] = risk_scores
    result_data['area_weight'] = data['population_density'] / data['population_density'].max()
    
    # è®¡ç®—åŠ æƒé£é™©è¯„åˆ†ï¼ˆè€ƒè™‘é¢ç§¯æƒé‡ï¼‰
    result_data['weighted_risk_score'] = result_data['risk_score'] * result_data['area_weight']
    
    # æ ‡å‡†åŒ–é£é™©è¯„åˆ†
    result_data['normalized_risk_score'] = (result_data['risk_score'] - result_data['risk_score'].min()) / (result_data['risk_score'].max() - result_data['risk_score'].min())
    result_data['normalized_weighted_risk_score'] = (result_data['weighted_risk_score'] - result_data['weighted_risk_score'].min()) / (result_data['weighted_risk_score'].max() - result_data['weighted_risk_score'].min())
    
    print(f"âœ… é£é™©è¯„åˆ†ç”Ÿæˆå®Œæˆ")
    print(f"   å¹³å‡é£é™©è¯„åˆ†: {risk_scores.mean():.2f}")
    print(f"   é£é™©è¯„åˆ†èŒƒå›´: {risk_scores.min():.2f} - {risk_scores.max():.2f}")
    print(f"   å¹³å‡åŠ æƒé£é™©è¯„åˆ†: {result_data['weighted_risk_score'].mean():.2f}")
    
    return result_data

def analyze_feature_importance(model, features):
    """
    åˆ†æç‰¹å¾é‡è¦æ€§
    """
    print("ğŸ”„ åˆ†æç‰¹å¾é‡è¦æ€§...")
    
    # è·å–ç‰¹å¾é‡è¦æ€§
    importance = model.feature_importances_
    feature_importance = pd.DataFrame({
        'feature': features,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    print(f"âœ… ç‰¹å¾é‡è¦æ€§åˆ†æ:")
    for _, row in feature_importance.iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")
    
    return feature_importance

def save_results(result_data, feature_importance):
    """
    ä¿å­˜ç»“æœ
    """
    print("ğŸ”„ ä¿å­˜ç»“æœ...")
    
    # ä¿å­˜é£é™©è¯„åˆ†
    risk_output = result_data[['subzone_code', 'subzone_name', 'planning_area', 
                              'latitude', 'longitude', 'Total_Total', 
                              'risk_score', 'weighted_risk_score', 
                              'normalized_risk_score', 'normalized_weighted_risk_score',
                              'area_weight']].copy()
    
    risk_output.to_csv("outputs/optimized_risk_scores_with_area.csv", index=False)
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§
    feature_importance.to_csv("outputs/risk_model_feature_importance.csv", index=False)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# é£é™©æ¨¡å‹ä¼˜åŒ–æŠ¥å‘Š - åŸºäºé¢ç§¯æƒé‡

## æ¨¡å‹æ¦‚è¿°
ä½¿ç”¨XGBoostå›å½’æ¨¡å‹é¢„æµ‹åˆ†åŒºé£é™©è¯„åˆ†ï¼Œè€ƒè™‘é¢ç§¯æƒé‡å½±å“ã€‚

## ç‰¹å¾å·¥ç¨‹
- åŸºç¡€ç‰¹å¾: äººå£æ€»æ•°ã€å¿—æ„¿è€…æ•°é‡ã€HDBæ¯”ä¾‹ã€è€å¹´æ¯”ä¾‹ã€ä½æ”¶å…¥æ¯”ä¾‹ã€AEDæ•°é‡
- æ–°å¢ç‰¹å¾: é¢ç§¯æƒé‡ï¼ˆåŸºäºäººå£å¯†åº¦æ ‡å‡†åŒ–ï¼‰

## æ¨¡å‹æ€§èƒ½
- è®­ç»ƒé›† RÂ²: {r2_score(y_train, y_pred_train):.4f}
- æµ‹è¯•é›† RÂ²: {r2_score(y_test, y_pred_test):.4f}
- è®­ç»ƒé›† RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}
- æµ‹è¯•é›† RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}

## é£é™©è¯„åˆ†ç»Ÿè®¡
- å¹³å‡é£é™©è¯„åˆ†: {result_data['risk_score'].mean():.2f}
- é£é™©è¯„åˆ†èŒƒå›´: {result_data['risk_score'].min():.2f} - {result_data['risk_score'].max():.2f}
- å¹³å‡åŠ æƒé£é™©è¯„åˆ†: {result_data['weighted_risk_score'].mean():.2f}

## é¢ç§¯æƒé‡è¯´æ˜
- ä½¿ç”¨äººå£å¯†åº¦ä½œä¸ºé¢ç§¯ä»£ç†
- é«˜å¯†åº¦åŒºåŸŸè·å¾—æ›´é«˜æƒé‡
- åŠ æƒé£é™©è¯„åˆ† = åŸå§‹é£é™©è¯„åˆ† Ã— é¢ç§¯æƒé‡

## ç‰¹å¾é‡è¦æ€§æ’åº
{feature_importance.to_string(index=False)}

## è¾“å‡ºæ–‡ä»¶
- optimized_risk_scores_with_area.csv: åŒ…å«åŸå§‹å’ŒåŠ æƒé£é™©è¯„åˆ†
- risk_model_feature_importance.csv: ç‰¹å¾é‡è¦æ€§åˆ†æ
"""
    
    with open("outputs/risk_model_with_area_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("âœ… ç»“æœä¿å­˜å®Œæˆ")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   - outputs/optimized_risk_scores_with_area.csv")
    print("   - outputs/risk_model_feature_importance.csv")
    print("   - outputs/risk_model_with_area_report.md")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ¯ é£é™©æ¨¡å‹ä¼˜åŒ– - åŸºäºé¢ç§¯æƒé‡")
    print("=" * 50)
    
    # åŠ è½½å’Œå‡†å¤‡æ•°æ®
    X, y, data, features = load_and_prepare_data()
    
    # è®­ç»ƒæ¨¡å‹
    model, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test = train_risk_model(X, y)
    
    # ç”Ÿæˆé£é™©è¯„åˆ†
    result_data = generate_risk_scores(model, X, data)
    
    # åˆ†æç‰¹å¾é‡è¦æ€§
    feature_importance = analyze_feature_importance(model, features)
    
    # ä¿å­˜ç»“æœ
    save_results(result_data, feature_importance)
    
    print("\nğŸ‰ åŸºäºé¢ç§¯æƒé‡çš„é£é™©æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    main() 